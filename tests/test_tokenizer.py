# -*- coding: utf-8 -*-

import os
import pytest

from langml.tokenizer import Encoding, SPTokenizer, WPTokenizer


data_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data')


@pytest.mark.parametrize(
    "left_text,right_text",
    [
        (
            'I like apples', None,
        ),
        (
            'æˆ‘å–œæ¬¢åƒè‹¹æœ', None
        ),
        (
            'I like apples', 'æˆ‘å–œæ¬¢åƒè‹¹æœ'
        ),
        (
            'ä½ å¥½å‘€\nğŸ‘‹', 'hello world!!!\nğŸ‘‹'
        ),
        (
            "My mom always said life was like a box of chocolates. You never know what you're gonna get.", None
        ),
        (
            '''This line irked me. It is from the movie â€˜Forrest Gumpâ€™ where the protagonist, played by Tom Hanks, quotes his mother:\nForrest Gump: My momma always said, â€œLife was like a box of chocolates. You never know what youâ€™re gonna get.â€\nYet, you do know, donâ€™t you? You are going to get a box of chocolates. Perhaps you wonâ€™t know the taste of the chocolate specifically, perhaps one is more minty, the other might have that little coffee-taste of sorts, but chocolates nonetheless.\nI thought about it yesterday in the shower and figured, sure, if you compare it like that everyone gets a box of chocolates. Some boxes are bigger, some are smaller. Some chocolate boxes probably only have one piece of chocolate in it if you are lucky, and women probably have 70% of the chocolate inside.\nUltimately though, you know what you are going to get: chocolate. I admit that if you are getting a box of chocolates gifted to you, yes, you will not know what is inside. Life is a gift after all, so it makes sense. Yet life in itself is not mysterious as such, it is portrayed as quite simple. We wander around looking for meaning behind the chocolate. What does the chocolate mean? What if my chocolate is bitter? I donâ€™t like the dark chocolate, I like it a little lighter, or white, or with caramel, or nuts. What if I donâ€™t like the chocolate?\nI was bothered by the idea that it could be so simple, yet I figure in the grand scheme of things, yes, life is like a box of chocolates. It is that simple. It appears mysterious, that box, on the outside. Who knows what kinds of chocolates are inside? Then you open it and see the forms and shapes of the various chocolates, but you donâ€™t know how it tastes. Take a bite and find out! The mystery unravels itself in time, as you are living.\nThatâ€™s life, taking a bite of that unknown piece of chocolate, but more than than simply taking a bite out of the chocolate and seeing how it tastes, but appreciating that taste. Not judging it for not being the taste you expected, or disappointed that it wasnâ€™t as tasty as it looked. Nor, should it be the case, being sad it only has 5 pieces instead of 10, or that the box isnâ€™t as big as youâ€™d hoped. Enjoy the piece of chocolate, every piece, and donâ€™t leave a single piece behind.\nLife is like a box of chocolates, so enjoy every bite.''', None  # NOQA
        ),
        (
            '''é©¬å°šé¾™ | ä¸Šæµ·æ¯ä¸€ä¸ªåŒºï¼Œå„ç”±ä¸€å­—æ¥å¾®ç¼© åŸåˆ› é©¬å°šé¾™ å¤§ä¸Šæµ·å°é¾™å¼„\n\nä¸€æ™ƒå·²ç»æ˜¯7å¹´å‰çš„äº‹æƒ…â€”â€”äººä¸€ç”Ÿé€šå¸¸ä¹Ÿå°±æ˜¯æ™ƒåå‡ æ™ƒã€‚ ğŸˆ³ï¸   \n   å°±æ˜¯è¿™ä¹ˆä¸€æ™ƒä¹‹é—´ï¼Œè®¸å¤šäººå’Œäº‹æ™ƒæ²¡äº†ï¼Œè®¸å¤šäººå’Œäº‹æ™ƒå‡ºæ¥äº†ã€‚æ¯”å¦‚ï¼Œæ™ƒç€æ™ƒç€ï¼Œé—¸åŒ—åŒºæ²¡äº†ï¼›æ™ƒç€æ™ƒç€ï¼Œä¸´æ¸¯æ–°ç‰‡åŒºæœ‰äº†ã€‚\n\næ‰¿è’™ä¸Šæµ·èˆªç©ºä¼ åª’çš„æŠ¬çˆ±ï¼Œæˆ‘æ‹…ä»»äº†ã€Šä¸Šæµ·é­…åŠ›ã€‹è¿™æœ¬ä¹¦çš„ä¸»ç¼–ã€‚æ­¤ä¹¦æ±‡é›†äº†ä¸Šæµ·åä¸ƒä¸ªåŒºå¿çš„äººæ–‡å†å²æ—…æ¸¸åœ°è²Œâ€”â€”2014å¹´å´‡æ˜å°šæœªæ’¤å¿ï¼Œé—¸åŒ—ä»æ˜¯ä¸€åŒºã€‚å½“æ—¶æˆ‘ä»¬ç»™è‡ªå·±æŒ–äº†ä¸€ä¸ªå‘ï¼Œåœ¨æ¯ä¸€ä¸ªåŒºå¿çš„å›¾æ–‡ç»„åˆä¹‹é¦–ï¼Œåšä¸€ä¸ªâ€œä¸€å­—è§£è¯»â€ï¼Œæ—¢è¦ç‚¹ç›ï¼Œåˆè¦æœ‰è¶£å’Œä¸ªæ€§åŒ–ï¼Œä¸æ±‚é¢é¢ä¿±åˆ°ï¼Œä½†æ±‚å†°å±±ä¸€è§’ï¼Œè€Œåå†ç”¨æçŸ­çš„æ–‡å­—è§£è¯»è¿™ä¸€ä¸ªå­—ã€‚\n\nåˆ›æ„å¾ˆå¥½ã€‚åªæ˜¯æ‰€æœ‰çš„åˆ›æ„éƒ½æ˜¯å‘ï¼Œå‘æŒ–å¥½äº†ï¼Œè°è¿›è¿™ä¸ªå‘å‘¢ï¼Ÿåªæœ‰è®©â€œä¸»ç¼–å¤§äººâ€è·³è¿›å»å§ã€‚\n\n\n\n\nå°é¢å›¾ç‰‡è¯´æ˜ï¼šä¼è ¡ç”«ç”»ï¼Œ1964å¹´ ä¸Šæµ·ç‚¼æ²¹å‚ä¸€è§’\n\n\näºæ˜¯æˆ‘å°±å†™ä¸‹äº†ã€Šä¸Šæµ·é­…åŠ›ã€‹åä¸ƒä¸ªâ€œä¸Šæµ·ä¹‹â€¦â€¦â€ã€‚ä¸ºåä¸ƒä¸ªåŒºå¿åˆ›ä½œæµ“ç¼©ç‰ˆç²¾åç”»ä½œçš„æ˜¯è‘—åç”»å®¶æˆ´çº¢å€©ã€‚æˆ‘å’Œä»–çš„æ„‰å¿«åˆä½œå§‹äºã€Šä¸Šæµ·åˆ¶é€ ã€‹ã€‚\n\nåä¸ƒä¸ªåŒºå¿çš„â€œä¸Šæµ·ä¹‹â€¦â€¦â€ï¼Œçº¯å±æˆ‘ä¸ªäººçš„â€œçä¸ƒæ­å…«â€ï¼Œè¯´ä¸ä¸Šå†å²ä¾æ®ï¼Œæ›´æ²¡æœ‰å®˜æ–¹æ„å›¾ã€‚åœ¨å½“æ—¶çš„ã€Šä¸Šæµ·èˆªç©ºã€‹å…ˆæœŸåˆŠç™»åï¼Œç«Ÿä¹Ÿæœ‰äººçœ‹åˆ°ï¼Œè¿˜å’Œæˆ‘è®¨è®ºã€‚æœ‰è¯´æˆ‘å†™å¾—å¦™çš„ï¼Œä¹Ÿæœ‰ä¸åŒæ„çš„ï¼Œæ¯”å¦‚æ¨æµ¦åŒºâ€œä¸Šæµ·ä¹‹é‡â€ï¼Œæ˜¯å¦åº”è¯¥å†™æˆâ€œä¸Šæµ·ä¹‹å­¦â€â€”â€”å¤§å­¦å¤šæ–‡åŒ–æ°›å›´æµ“ã€‚ä¸è¿‡æˆ‘å°±ä¸€æ„å­¤è¡Œï¼Œä¸€ä¸ªäººâ€œä¸€â€åˆ°åº•äº†ã€‚æœ¬èº«å°±æ˜¯éšæ„ä¸ºä¹‹ï¼Œä¸€è®¨è®ºä¸€å¾æ±‚æ„è§ï¼Œå¥½åƒæ˜¯é©å‘½æ ·æ¿æˆé‡ç‚¹é¢˜æçš„åˆ›ä½œå¥—è·¯äº†ã€‚\n\næ¾æ±Ÿï¼šä¸Šæµ·ä¹‹æ ¹\n\n\n\nå¦‚æœè¯´æ³°æ™¤å£«å°é•‡æ˜¯æ¾æ±Ÿçš„æ—¶å°šï¼Œå¦‚æœè¯´å¤§å­¦åŸæ˜¯æ¾æ±Ÿçš„å¿ƒèƒ¸ï¼Œé‚£ä¹ˆå¹¿å¯Œæ—æ˜¯ä¸Šæµ·çš„éª„å‚²ï¼Œå½“å¹¿å¯Œæ—è¢«æ³¨æ°´é‡ç°ä¸Šæµ·ä¹‹åˆæ—¶ï¼Œåœ¨æ°´ä¸€æ–¹æ˜¯ä¸Šæµ·çš„æ ¹ã€‚\n\né™å®‰ï¼šä¸Šæµ·ä¹‹ç§€\n\n\n\né™æ˜¯æ€åº¦ï¼Œå®‰æ˜¯ç¥ˆç¦ï¼›æœ€è¢–ççš„å…¬å›­é‡Œæœ‰æœ€ç§€é›…çš„æ¢§æ¡ï¼Œæœ€æ‹¥æŒ¤çš„äººç¾¤ï¼Œæ— ç–‘ä¹Ÿæ˜¯æœ€ç§€æ…§çš„äººæ–‡æ™¯è§‚ï¼Œä¸€å¹´ä¸€åº¦çš„ä¸Šæµ·ä¹¦å±•ï¼Œä¸€å†åˆ›é€ ç€ä¸­å›½å›¾ä¹¦å±•è§ˆçš„çºªå½•ã€‚\n\nå˜‰å®šï¼šä¸Šæµ·ä¹‹é€Ÿ\n\n\n\næœ€æé€Ÿçš„æ˜¯F1ï¼Œå´ä¸å®Œå…¨æ˜¯ï¼Œè¿˜æœ‰ä¸­å›½ç¬¬ä¸€æ¡é«˜é€Ÿå…¬è·¯æ²ªå˜‰é«˜é€Ÿï¼Œè¿˜æœ‰ç¬¬ä¸€ä¸ªä¸­å¾·åˆèµ„çš„æ±½è½¦å“ç‰Œå¤§ä¼—æ¡‘å¡”çº³ï¼Œæ›´æœ‰ä»¥æ­¤å¸¦æ¥çš„å˜‰å®šé€Ÿåº¦ã€‚\n\næ¨æµ¦ï¼šä¸Šæµ·ä¹‹é‡\n\n\n\né‡å¯ä»¥æ˜¯å·¥ä¸šï¼Œä¸Šæµ·çš„é‡å·¥ä¸šåŸºåœ°å†æ¬¡èˆæˆ‘å…¶è°ï¼Ÿé‡ä¹Ÿå¯ä»¥æ˜¯å­¦ä¸šï¼Œä¸Šæµ·çš„é‡ç‚¹å¤§å­¦åœ¨æ­¤è”è¢‚ï¼›é‡è¿˜å¯ä»¥æ˜¯å†å²ï¼Œè¯¸å¤šé‡è¦çš„äº‹ä»¶åœ¨æ­¤å±•å¼€ã€‚\n\né‡‘å±±ï¼šä¸Šæµ·ä¹‹ç„°\n\n\n\næµ·ä¸Šå‡æ˜æœˆæ˜¯å”è¯—çš„æ„å¢ƒï¼Œæµ·ä¸Šå‡ç„°ç«æ˜¯é‡‘å±±çš„èŠ‚æ—¥ï¼Œæ¸”æ‘çš„å‘³é“ï¼Œå•¤é…’èŠ‚çš„å–§é—¹ï¼ŒéŸ³ä¹èŠ‚çš„å£°é“â€¦â€¦å¯ä»¥é—²æ­¥ï¼Œå¯ä»¥é—²èŠï¼Œå¯ä»¥é—²é›…ã€‚\n\nè™¹å£ï¼šä¸Šæµ·ä¹‹çµ\n\n\n\næ°‘æ—çš„çµé­‚åœ¨è¿™é‡Œå‘å‡ºå‘å–Šï¼Œæ–‡å›å¤§å¸ˆçš„çµæ„Ÿè‡³ä»Šå›è¡ï¼›è¿™é‡Œä¼ é€’ç€ä»é‚®æ”¿æ€»å±€å‘å‡ºçš„ä¿¡å‡½ï¼Œä»ç”œçˆ±è·¯æ•£å‘çš„çˆ±æƒ…ä¼ è¯´ã€‚\n\nå´‡æ˜ï¼šä¸Šæµ·ä¹‹ç»¿\n\n\n\næ›¾ç»çš„è’èŠœåŒ–ä¸ºå…¨ä¸Šæµ·è§„æ¨¡æœ€å¤§çš„ç”Ÿæ€æ¹¿åœ°ï¼Œæ›¾ç»çš„è§’è½åŒ–ä¸ºå…¨ä¸Šæµ·ç©ºæ°”æœ€æ¸…æ–°çš„å¤©ç„¶æ°§å§ï¼Œæ›¾ç»çš„æœå‘å¤•è‡³åŒ–ä¸ºæ¡¥éš§ä¸€è·¯é€šã€‚\n\né•¿å®ï¼šä¸Šæµ·ä¹‹è™¹\n\n\n\nå†¥å†¥ä¸­çš„å‘½åï¼Œå¸¦æ¥äº†å†¥å†¥ä¸­çš„ä½¿å‘½ï¼Œè™¹æ¡¥ï¼Œæ˜¯ä¸Šæµ·æœ€æ—©é€šå‘å…¨ä¸–ç•Œçš„å¤©è™¹ä¹‹æ¡¥ï¼Œä¹Ÿæ˜¯é€šå‘å®é™ã€é€šå‘æ–‡æ˜ã€é€šå‘å¯Œè£•çš„é•¿è™¹ä¹‹æ¡¥ã€‚\n\nå¾æ±‡ï¼šä¸Šæµ·ä¹‹å½±\n\n\n\nè¿™é‡Œï¼Œæœ‰ä¸Šæµ·ç”µå½±åˆ¶ç‰‡å‚ç•™ä¸‹çš„å…‰å½±ï¼›æœ‰è—ä¹¦æ¥¼ç•™ä¸‹çš„æ–‡åŒ–çš„èƒŒå½±ï¼›æœ‰åœŸå±±æ¹¾ç•™ä¸‹çš„å†å²çš„å€’å½±ï¼›æœ‰å¾å®¶æ±‡å•†åœˆç•™ä¸‹çš„ç†™æ”˜äººç¾¤çš„æ–œå½±ã€‚\n\nå®å±±ï¼šä¸Šæµ·ä¹‹æ°´\n\n\n\nä¸œæµ·ã€é•¿æ±Ÿå’Œé»„æµ¦æ±Ÿä¸‰æ°´åˆä¸€ï¼Œæ˜¯è‘—åçš„â€œä¸‰å¤¹æ°´â€ï¼Œä¸‰å¤¹æ°´å°±åœ¨å®å±±çš„å´æ·å£å›æ—‹ï¼›ä¸‰æ°´åœ¨æ­¤æ±‡åˆï¼Œå´æ˜¯ä»¥è‡ªå·±çš„é¢œè‰²è€ŒåŒºåˆ†å½¼æ­¤ã€‚æµ·çº³ç™¾å·æœ‰äº†ä¸€ä¸ªç»å¥½çš„æ³¨é‡Šã€‚å¦‚ä»Šçš„é‚®è½®æ°æ˜¯è¢«ä¸‰å¤¹æ°´æ¶Œå‘è¿œæ–¹ã€‚\n\næ™®é™€ï¼šä¸Šæµ·ä¹‹ç‰\n\n\n\nç‰ä½›å¯ºå¤–ä¸¤æ¡è·¯çš„è·¯åï¼Œä¸ç»æ„é—´ä½“ç°äº†æœ€é«˜çš„ç¦…æ„â€”â€”å—åŒ—å‘çš„æ±Ÿå®è·¯ï¼Œä¸œè¥¿å‘çš„å®‰è¿œè·¯â€”â€”å®é™è€Œè‡´è¿œï¼›æˆ–è®¸ï¼Œä¹Ÿæ˜¯ç‰çš„å¢ƒç•Œå§ã€‚å…¶å®è¿™åˆä½•å°ä¸æ˜¯ä¸Šæµ·äººçš„ç”Ÿæ´»å‘å¾€ï¼Ÿ\n\né’æµ¦ï¼šä¸Šæµ·ä¹‹æ³½\n\n\n\næ´‹æ´‹å¤§è§‚æ·€å±±æ¹–ï¼Œå°æ¡¥æµæ°´æœ±å®¶è§’ï¼Œå°¤å…¶æ˜¯å¼•ä»¥ä¸ºè£çš„å´§æ³½æ–‡åŒ–ï¼Œå®ä¼Ÿçš„ç¯‡å¹…å¯ä»¥å±•ç¤ºï¼Œä¼ å¥‡çš„æ•…äº‹å¯ä»¥æµä¼ ï¼Œæµªæ¼«çš„é£æƒ…å¯ä»¥æ¼”ç»â€”â€”å°±å› ä¸ºæ˜¯é±¼ç±³ä¹‹ä¹¡å’Œäººæ–‡æ•…åœ°ã€‚å¥¥ç‰¹è±æ–¯ï¼Œç¦å¯¿å›­ï¼Œä¹ƒè‡³æ‰è‚‰å¤§ç±³ï¼Œè¿¥å¼‚è€Œå£°åè¿œæ‰¬ã€‚\n\né—¸åŒ—ï¼šä¸Šæµ·ä¹‹é­”\n\n\n\næ›¾ç»æœ‰è¿‡ç»å…¸é­”æœ¯ï¼Œå°†ä¸¤æ¬¡æ·æ²ªæˆ˜äº‰æ—¶æœŸè¢«æ—¥å†›ç‚¸æ¯çš„ç«è½¦ç«™è¿…é€Ÿä¿®å¤ï¼›æ›¾ç»æœ‰è¿‡å¤å½©é­”æœ¯ï¼ŒæŠŠæ»šåœ°é¾™å˜æˆäº†æ–°å·¥æˆ¿ï¼›æ›¾ç»æœ‰è¿‡æ—¶å°šé­”æœ¯ï¼Œé©¬æˆåŸæˆä¸ºä¸Šæµ·æ—…æ¸¸çš„é‡å¤´æˆï¼›è¿˜å°†æ‹‰å¼€æœªæ¥é­”æœ¯ï¼Œé—¸åŒ—åŒºèå…¥äº†é™å®‰åŒºã€‚\n\né—µè¡Œï¼šä¸Šæµ·ä¹‹å®\n\n\n\né—µè¡Œæ›¾ç»ç¦»ä¸Šæµ·å¸‚ä¸­å¿ƒæå…¶é¥è¿œï¼Œåœ¨é—µè¡Œä¸Šç­çš„å·¥äººéƒ½ä¸èƒ½æ¯å¤©å›å®¶ï¼Œå¦‚ä»Šé—µè¡Œæœ¬èº«å°±æ˜¯å±…æ°‘äº‘é›†ã€‚ä¸ƒå®ï¼Œæ›¾ç»ä¹…é—»ä¸ƒå®å¤§æ›²ï¼Œå´æ— æ³•ä»å®¹æ¥å»ï¼Œå¦‚ä»Šçš„ä¸ƒå®å¤é•‡ï¼Œä¹Ÿæ— æ³•ä»å®¹æ¥å»ï¼Œåªå› æ¸¸äººä¸ºä¸ƒå®è€Œå»ã€‚\n\nå¥‰è´¤ï¼šä¸Šæµ·ä¹‹æ¹¾\n\n\n\næµ·æ¹¾æ˜¯è‡ªç„¶çš„åœ°ç†æ€åŠ¿ï¼Œè“å¤©ç¢§æµ·ï¼Œä¸€è§ˆæ— ä½™ï¼›æµ·æ¹¾ä¹Ÿæ˜¯äººå·¥çš„åˆ›é€ ï¼Œç¢§æµ·é‡‘æ²™ï¼Œæ˜¯è§‚æµ·ï¼Œä¹Ÿæ˜¯å¬‰æ²™ã€‚æµ·æ¹¾æ›´æ˜¯äººæ–‡çš„æ¸¯æ¹¾ï¼Œâ€œå¥‰è´¤â€äºŒå­—ï¼Œä¾¿æ˜¯å‘ŠçŸ¥å¤©ä¸‹ï¼Œç¤¾ä¼šè´¤è¾¾ï¼Œåœ¨è¿™ä¸€ä¸ªæ¸¯æ¹¾çš„åœ°ä½ã€‚\n\né»„æµ¦ï¼šä¸Šæµ·ä¹‹æ›¦\n\n\n\nå¦‚æœè¯´ï¼Œæ›¦æ˜¯æ¯ä¸€å¤©é˜³å…‰ä¹‹ç¬¬ä¸€ï¼Œé‚£ä¹ˆï¼Œå—äº¬è·¯å°±æ˜¯ç¬¬ä¸€ï¼Œå›½é™…é¥­åº—å°±æ˜¯ç¬¬ä¸€ï¼Œæ·®æµ·è·¯å°±æ˜¯ç¬¬ä¸€ï¼Œä¸­å›½äººè‡ªå·±åˆ›åŠçš„è‡ªæ¥æ°´å‚å°±æ˜¯ç¬¬ä¸€â€¦â€¦è®¸å¤šå†å²æ·±è¿œçš„æ”¿æ²»æ´»åŠ¨ä»æ˜¯ç¬¬ä¸€ï¼Œæ¯”å¦‚ï¼Œä¸€å¤§ä¼šå€ã€‚\n\næµ¦ä¸œï¼šä¸Šæµ·ä¹‹ç \n\n\n\næœ€åˆï¼Œä¸œæ–¹æ˜ç åªæ˜¯ä¸€åº§ç”µè§†å¡”ï¼Œåæ¥ï¼Œå®ƒæˆä¸ºäº†æ—…æ¸¸çƒ­åœ°ï¼Œå†åæ¥å®ƒå·²ç»ä¸å†æ˜¯ç”µè§†å¡”ï¼Œä¸å†æ˜¯æœ€é«˜ï¼Œä½†æ˜¯å®ƒæˆä¸ºäº†æµ¦ä¸œçš„åŒåè¯ï¼Œæˆä¸ºäº†æµ¦ä¸œçš„â€œä»£è¨€äººâ€ï¼Œæµ¦ä¸œå°±æ˜¯ä¸œæ–¹ä¹‹ç ã€‚\n\n\n\nã€Šä¸Šæµ·é­…åŠ›ã€‹å‡ºç‰ˆåï¼Œæ–°çš„åˆ›æ„è¯ç”Ÿï¼Œé‚£å°±æ˜¯åæ¥çš„ã€Šçˆ±ä¸Šæµ·ã€‹æ˜ä¿¡ç‰‡çè—ç‰ˆï¼Œä»…600å†Œï¼Œä¸”ç”±æˆ´æ•¦é‚¦å¤§å¸ˆé¢˜â€œçˆ±ä¸Šæµ·â€ã€‚æ˜ä¿¡ç‰‡å‡ºç‰ˆæ—¶ï¼Œå´‡æ˜æ’¤å¿æ”¹åŒºï¼Œé—¸åŒ—å¹¶å…¥é™å®‰ï¼Œå†æ²¡æœ‰åä¸ƒä¸ªåŒºå¿ä¹‹è¯´ï¼Œæ‰€ä»¥ã€Šçˆ±ä¸Šæµ·ã€‹æ˜¯çœŸæ­£çš„ç»ç‰ˆäº†ã€‚\n\nè½®ä¸åˆ°æˆ‘ç”»é¾™ç‚¹ç›ï¼Œä¹ŸæŒ¨ä¸ç€æˆ‘ç”»è›‡æ·»è¶³ï¼Œæˆ‘åªæ˜¯â€œå¤§ä¸Šæµ·å°é¾™å¼„â€ä¸€ç•ªè€Œå·²ï¼ŒæµªèŠ±ä¹Ÿä¸èµ·çš„ã€‚\n\næœ¬æ¬¡å…¬å·æ–‡å­—å¤ªå°‘äº†ï¼Œå°±è´´ä¸¤æ®µæœ‰å…³ä¸Šæµ·çš„æ–‡å­—å§ã€‚è¿™æ˜¯æˆ‘å†™äº2009å¹´ã€Šæµ·æ´¾æ ¼è°ƒã€‹çš„æ®µè½â€”â€”\n\næœ‰å¾ˆå¤šäº‹æƒ…ï¼Œæ˜¯å¦å‘ç”Ÿåœ¨ä¸Šæµ·å°±ä¼šæœ‰ä¸åŒçš„ç»“æœï¼›å…¶ä»–åœ°åŸŸå½“ç„¶ä¹Ÿä¼šæœ‰ç±»ä¼¼çš„ç°è±¡ï¼Œä½†æ˜¯ä¸è‡³äºåƒä¸Šæµ·ä¸€æ ·ï¼Œè¿™æ ·çš„äº‹æƒ…ä¼šå¾ˆå¤šï¼Œæ•ˆæœä¼šå¾ˆå¼ºçƒˆã€‚å°¤å…¶æ˜¯å½“è¿™æ ·çš„äº‹æƒ…å¾ˆå°ï¼Œæœ¬ä¸åº”è¯¥ä¼šæœ‰å¼ºçƒˆçš„åå“ï¼Œåœ¨å…¶ä»–åœ°åŸŸç”šè‡³å°±ä¸ä¼šæˆä¸ºä¸€ä»¶äº‹æƒ…ï¼Œåœ¨ä¸Šæµ·å´èƒ½æˆä¸ºä¸€ä¸ªå®¶å–»æˆ·æ™“çš„æ–°é—»äº‹ä»¶ã€‚ä¸€åº§æ¡¥çš„ä¿®æ•´ï¼Œä¸€åº§çƒŸå›±çš„ä¿ç•™ï¼Œå¯ä»¥ååå¤å¤æˆä¸ºé¥­æ¡Œä¸Šçš„è°ˆèµ„ã€‚\n\n\n\nä¸Šæµ·çš„å¤–ç™½æ¸¡æ¡¥è¦åšè¿ç§»å¼å¤§ä¿®ï¼ŒåŸæœ¬è¿™å°±æ˜¯ä¸€ä¸ªå¸‚æ”¿å·¥ç¨‹ï¼Œè‹å·æ²³ä¸Šæ‰€æœ‰çš„æ¡¥éƒ½ç»å†äº†ä¿®æ•´ï¼Œå½“ç„¶å¤–ç™½æ¸¡æ¡¥äº«å—çš„æ˜¯æœ€ç‰¹æ®Šçš„å¾…é‡ï¼Œè¿ç§»å¤§ä¿®ï¼Œè€Œéæ‹†æ—§é€ æ–°ã€‚è¿™ä¸€é¡¹å¸‚æ”¿å·¥ç¨‹ä»å‘å¸ƒæ¶ˆæ¯ç›´è‡³æ­£å¼è¿ç§»ï¼Œä¸€ç›´æ˜¯ä¸Šæµ·æ–‡åŒ–ç•Œçš„çƒ­è®®è¯é¢˜ï¼Œè¯¸å¦‚åœ°æ ‡å¼çš„æ„ä¹‰ï¼Œæ—§ä¸Šæµ·çš„é—¨æˆ·ï¼Œâ€œåäººä¸ç‹—ä¸å¾—å…¥å†…â€çš„è§è¯â€¦â€¦æœ€æœ‰æ„æ€çš„æ˜¯ï¼Œæ—¶å°šç•Œå§‹ç»ˆå°†å®ƒä½œä¸ºä¸€ä¸ªæ—¶å°šäº‹ä»¶ï¼Œå›´ç»•å¤–ç™½æ¸¡æ¡¥çš„å¤§ä¿®ï¼Œåœ¨å¤–ç™½æ¸¡æ¡¥ä¸Šä¸¾è¡Œä¸€ç³»åˆ—æ—¶å°šæ´»åŠ¨ï¼Œå½±æ˜Ÿç« å­æ€¡å°±åœ¨å¤–ç™½æ¸¡æ¡¥ä¸Šæ‹è¿‡ä¸€ç»„ç…§ç‰‡ã€‚è¿˜æœ‰äººå‘æ•£æ€§æ€ç»´åœ°åˆ›æ„ï¼šå°†å¤–ç™½æ¸¡æ¡¥ä¸Šæ‹†ä¸‹æ¥çš„åºŸæ—§é“†é’‰åµŒåœ¨æ°´æ™¶ç»ç’ƒå†…ï¼Œä¸€å®šæ˜¯å¾ˆæœ‰æ„ä¹‰çš„çºªå¿µå“å’Œè£…é¥°å“ï¼Œåæ¥æœç„¶æœ‰äº†è¿™ä»½é•¶åµŒäº†åºŸé“œçƒ‚é“çš„çºªå¿µå“ã€‚æ˜¯ä¸Šæµ·è¿™åº§åŸå¸‚çš„æ„ä¹‰ï¼Œå†³å®šäº†ä¸€åº§è€æ¡¥çš„åœ°ä½ã€‚\n\n\n\nåŒæ ·çš„æ„ä¹‰ä¹Ÿå†³å®šäº†ä¸€åº§çƒŸå›±çš„ä¿ç•™ã€‚åœ¨éå¸¸æ³¨é‡ç¯å¢ƒä¿æŠ¤çš„ä¸Šæµ·å¸‚ä¸­å¿ƒå¾å®¶æ±‡ç»¿åœ°ï¼Œä¸ä»…ä¿ç•™äº†å°æ´‹æˆ¿ï¼Œè€Œä¸”è¿˜ä¿ç•™äº†å·¥ä¸šç¤¾ä¼šè±¡å¾çš„çƒŸå›±ã€‚å‡ åå¹´å‰ï¼Œè¿™ä¸€åº§é»‘çƒŸæ»šæ»šçš„çƒŸå›±æ˜¯ä¼Ÿå¤§çš„è±¡å¾ï¼Œå°å­¦ç”Ÿå°†å®ƒå†™è¿›ä½œæ–‡é‡Œï¼Œç”»è¿›ç´ æé‡Œï¼›å‡ åå¹´åï¼Œå®ƒé™é™åœ°çŸ—ç«‹åœ¨ç»¿åœ°ä¸­ï¼Œä¾ç„¶éå¸¸é›„æ€§ï¼Œä¸ä»–åŒ¹é…çš„ä¸å†æ˜¯ç²—çŠ·çš„å·¥å‚ï¼Œè€Œæ˜¯æŸ”æƒ…çš„æ—¶å°šï¼Œå°å­©å­ä»¬åœ¨çƒŸå›±åº•ä¸‹æ¸¸æˆæ‹ç…§ï¼Œâ€œå¬å¦ˆå¦ˆè®²è¿‡å»çš„æ•…äº‹â€ã€‚\n\nåªæœ‰åœ¨çƒŸå›±å·²ç»å±äºâ€œè¿‡å»çš„æ•…äº‹â€çš„æ—¶å€™ï¼Œå®ƒçš„å±æ€§æ‰ä¼šç”±å·¥ä¸šæ¼”å˜ä¸ºæ–‡åŒ–å’Œæ—¶å°šï¼›å‡å¦‚è¿™ä¸€ä¸ªåŸå¸‚ä¾ç„¶è¿˜é¥±å—æ—ç«‹çš„çƒŸå›±å’Œæ»šæ»šçš„é»‘çƒŸï¼Œé‚£ä¹ˆä»»ä½•ä¸€åº§çƒŸå›±éƒ½æ˜¯ç½ªæ¶ã€‚æ˜¯ä¸Šæµ·è¿™ä¸€åº§åŸå¸‚çš„æ„ä¹‰ï¼Œæ”¹å˜äº†è¿™ä¸€åº§çƒŸå›±çš„æ˜¯éã€‚\n\né©¬å°šé¾™\n\nä¸­å›½ä½œå®¶åä¼šä¼šå‘˜ï¼Œä¸Šæµ·ä½œå®¶åä¼šç†äº‹ã€æ•£æ–‡æŠ¥å‘Šæ–‡å­¦ä¸“ä¸šåˆ›ä½œå§”å‘˜ä¼šå‰¯ä¸»ä»»ï¼›ç¼–å®¡\n\næ°‘è¿›ä¸Šæµ·å¸‚å§”å‡ºç‰ˆä¼ åª’å§”å‘˜ä¼šå‰¯ä¸»ä»»\n\nä¸Šæµ·é»„æµ¦åŒºæ˜å¤å›¾ä¹¦é¦†ç†äº‹é•¿\n\nä¸Šæµ·è¯„å¼¹å›¢è‰ºå§”ä¼šé¡¾é—®\n\nè‘—ä½œä¸»è¦åˆ†ä¸ºä¸‰ä¸ªç³»åˆ—ï¼Œåˆ†åˆ«æ˜¯ã€Šå¹½é»˜åº”ç¬‘æˆ‘ã€‹ã€Šä¸åäººåŒçª—ã€‹ç­‰æ‚æ–‡ç³»åˆ—ï¼Œã€Šä¸Šæµ·åˆ¶é€ ã€‹ã€Šä¸ºä»€ä¹ˆæ˜¯ä¸Šæµ·ã€‹ã€Šä¸Šæµ·åˆ†å¯¸ã€‹ã€Šä¸Šæµ·å¥³äººã€‹ã€Šä¸Šæµ·è·¯æ•°ã€‹ç­‰ä¸Šæµ·ç³»åˆ—ï¼Œã€Šå·æ‰‹è¯­ã€‹ã€Šæœ‰äº›æ„æ€ä½ ä»æ¥ä¸æ‡‚ã€‹ç­‰éšç¬”ç³»åˆ— ã€‚\n\n\n\næˆ‘çš„æ–°ä¹¦ã€Šä¸Šæµ·åˆ†å¯¸ã€‹ï¼Œä¸Šæµ·ä¹¦åº—å‡ºç‰ˆç¤¾2021å¹´1æœˆå‡ºç‰ˆã€‚\n\næ¬¢è¿å…³æ³¨æˆ‘çš„å¾®ä¿¡å…¬ä¼—å·â€œå¤§ä¸Šæµ·å°é¾™å¼„â€â€”â€”\n\nåŸæ ‡é¢˜ï¼šã€Šé©¬å°šé¾™ | ä¸Šæµ·æ¯ä¸€ä¸ªåŒºï¼Œå„ç”±ä¸€å­—æ¥å¾®ç¼©ã€‹''', None  # NOQA
        )
    ]
)
def test_wordpiece_encode(left_text, right_text):
    # lowercase=True
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)
    raw_tokenizer = tokenizer.raw_tokenizer()
    ret1 = tokenizer.encode(left_text, right_text, return_array=False)
    ret2 = raw_tokenizer.encode(left_text, right_text)

    assert ret1.ids == ret2.ids
    assert ret1.segment_ids == ret2.type_ids
    assert ret1.tokens == ret2.tokens

    # lowercase=False
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=False)
    raw_tokenizer = tokenizer.raw_tokenizer()
    ret1 = tokenizer.encode(left_text, right_text, return_array=False)
    ret2 = raw_tokenizer.encode(left_text, right_text)

    assert ret1.ids == ret2.ids
    assert ret1.segment_ids == ret2.type_ids
    assert ret1.tokens == ret2.tokens


@pytest.mark.parametrize(
    'inputs,padding_strategy,expected',
    [
        (
            [('I like apples', 'æˆ‘å–œæ¬¢åƒè‹¹æœ'), ('hello world!', 'ä½ å¥½ä¸–ç•ŒğŸ‘‹')],
            'post',
            Encoding(
                ids=[[101, 151, 8993, 8350, 8118, 102, 2769, 1599, 3614, 1391, 5741, 3362, 102],
                     [101, 8701, 8572, 106, 102, 872, 1962, 686, 4518, 100, 102, 0, 0]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]],
                tokens=[['[CLS]', 'i', 'like', 'apple', '##s', '[SEP]', 'æˆ‘', 'å–œ', 'æ¬¢', 'åƒ', 'è‹¹', 'æœ', '[SEP]'],
                        ['[CLS]', 'hello', 'world', '!', '[SEP]', 'ä½ ', 'å¥½', 'ä¸–', 'ç•Œ', '[UNK]', '[SEP]', '[PAD]', '[PAD]']]   
            ),
        ), (
            [('I like apples', 'æˆ‘å–œæ¬¢åƒè‹¹æœ'), ('hello world!', 'ä½ å¥½ä¸–ç•ŒğŸ‘‹')],
            'pre',
            Encoding(
                ids=[[101, 151, 8993, 8350, 8118, 102, 2769, 1599, 3614, 1391, 5741, 3362, 102],
                     [0, 0, 101, 8701, 8572, 106, 102, 872, 1962, 686, 4518, 100, 102]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]],
                tokens=[['[CLS]', 'i', 'like', 'apple', '##s', '[SEP]', 'æˆ‘', 'å–œ', 'æ¬¢', 'åƒ', 'è‹¹', 'æœ', '[SEP]'],
                        ['[PAD]', '[PAD]', '[CLS]', 'hello', 'world', '!', '[SEP]', 'ä½ ', 'å¥½', 'ä¸–', 'ç•Œ', '[UNK]', '[SEP]']]
            )
        )
    ]
)
def test_wordpiece_batch_encode_padding(inputs, padding_strategy, expected):
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)
    pred = tokenizer.encode_batch(inputs, padding=True, padding_strategy=padding_strategy, return_array=False)
    assert pred.ids == expected.ids
    assert pred.segment_ids == expected.segment_ids
    assert pred.tokens == expected.tokens


@pytest.mark.parametrize(
    'inputs,padding_strategy,expected',
    [
        (
            ['I like apples', 'æˆ‘å–œæ¬¢åƒè‹¹æœ', 'hello world!', 'ä½ å¥½ä¸–ç•ŒğŸ‘‹'],
            'post',
            Encoding(
                ids=[[101, 151, 8993, 8350, 8118, 102, 0, 0],
                     [101, 2769, 1599, 3614, 1391, 5741, 3362, 102],
                     [101, 8701, 8572, 106, 102, 0, 0, 0],
                     [101, 872, 1962, 686, 4518, 100, 102, 0]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0]],
                tokens=[['[CLS]', 'i', 'like', 'apple', '##s', '[SEP]', '[PAD]', '[PAD]'],
                        ['[CLS]', 'æˆ‘', 'å–œ', 'æ¬¢', 'åƒ', 'è‹¹', 'æœ', '[SEP]'],
                        ['[CLS]', 'hello', 'world', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]'],
                        ['[CLS]', 'ä½ ', 'å¥½', 'ä¸–', 'ç•Œ', '[UNK]', '[SEP]', '[PAD]']]  
            ),
        ), (
            ['I like apples', 'æˆ‘å–œæ¬¢åƒè‹¹æœ', 'hello world!', 'ä½ å¥½ä¸–ç•ŒğŸ‘‹'],
            'pre',
            Encoding(
                ids=[[0, 0, 101, 151, 8993, 8350, 8118, 102],
                     [101, 2769, 1599, 3614, 1391, 5741, 3362, 102],
                     [0, 0, 0, 101, 8701, 8572, 106, 102],
                     [0, 101, 872, 1962, 686, 4518, 100, 102]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0]],
                tokens=[['[PAD]', '[PAD]', '[CLS]', 'i', 'like', 'apple', '##s', '[SEP]'],
                        ['[CLS]', 'æˆ‘', 'å–œ', 'æ¬¢', 'åƒ', 'è‹¹', 'æœ', '[SEP]'],
                        ['[PAD]', '[PAD]', '[PAD]', '[CLS]', 'hello', 'world', '!', '[SEP]'],
                        ['[PAD]', '[CLS]', 'ä½ ', 'å¥½', 'ä¸–', 'ç•Œ', '[UNK]', '[SEP]']]
            )
        )
    ]
)
def test_wordpiece_batch_encode_single(inputs, padding_strategy, expected):
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)
    pred = tokenizer.encode_batch(inputs, padding=True, padding_strategy=padding_strategy, return_array=False)
    assert pred.ids == expected.ids
    assert pred.segment_ids == expected.segment_ids
    assert pred.tokens == expected.tokens


@pytest.mark.parametrize(
    'inputs,expected',
    [
        (
            [('I like apples', 'æˆ‘å–œæ¬¢åƒè‹¹æœ'), ('hello world!', 'ä½ å¥½ä¸–ç•ŒğŸ‘‹')],
            Encoding(
                ids=[[101, 151, 8993, 8350, 8118, 102, 2769, 1599, 3614, 1391, 5741, 3362, 102],
                     [101, 8701, 8572, 106, 102, 872, 1962, 686, 4518, 100, 102]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]],
                tokens=[['[CLS]', 'i', 'like', 'apple', '##s', '[SEP]', 'æˆ‘', 'å–œ', 'æ¬¢', 'åƒ', 'è‹¹', 'æœ', '[SEP]'],
                        ['[CLS]', 'hello', 'world', '!', '[SEP]', 'ä½ ', 'å¥½', 'ä¸–', 'ç•Œ', '[UNK]', '[SEP]']]   
            ),
        )
    ]
)
def test_wordpiece_batch_encode_no_padding(inputs, expected):
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)
    pred = tokenizer.encode_batch(inputs, padding=False, return_array=False)
    assert pred.ids == expected.ids
    assert pred.segment_ids == expected.segment_ids
    assert pred.tokens == expected.tokens


@pytest.mark.parametrize(
    'inputs,padding_strategy,expected',
    [
        (
            [('I like apples', 'æˆ‘å–œæ¬¢åƒè‹¹æœ'), ('hello world!', 'ä½ å¥½ä¸–ç•ŒğŸ‘‹')],
            'post',
            Encoding(
                ids=[[101, 151, 8993, 8350, 8118, 102, 2769, 1599, 3614, 102],
                     [101, 8701, 8572, 106, 102, 872, 1962, 686, 4518, 102]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]],
                tokens=[['[CLS]', 'i', 'like', 'apple', '##s', '[SEP]', 'æˆ‘', 'å–œ', 'æ¬¢', '[SEP]'],
                        ['[CLS]', 'hello', 'world', '!', '[SEP]', 'ä½ ', 'å¥½', 'ä¸–', 'ç•Œ', '[SEP]']]   
            ),
        ),
        (
            [('I like apples', 'æˆ‘å–œæ¬¢åƒè‹¹æœ'), ('hello world!', 'ä½ å¥½ä¸–ç•ŒğŸ‘‹')],
            'pre',
            Encoding(
                ids=[[101, 151, 8993, 8350, 8118, 102, 2769, 1599, 3614, 102],
                     [101, 8701, 8572, 106, 102, 872, 1962, 686, 4518, 102]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]],
                tokens=[['[CLS]', 'i', 'like', 'apple', '##s', '[SEP]', 'æˆ‘', 'å–œ', 'æ¬¢', '[SEP]'],
                        ['[CLS]', 'hello', 'world', '!', '[SEP]', 'ä½ ', 'å¥½', 'ä¸–', 'ç•Œ', '[SEP]']]   
            ),
        )
    ]
)
def test_wordpiece_batch_encode_truncation(inputs, padding_strategy, expected):
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)
    tokenizer.enable_truncation(max_length=10)
    pred = tokenizer.encode_batch(inputs, padding_strategy=padding_strategy, return_array=False)
    assert pred.ids == expected.ids
    assert pred.segment_ids == expected.segment_ids
    assert pred.tokens == expected.tokens


@pytest.mark.parametrize(
    "test_input,skip_special_tokens,expected",
    [
        ([101, 872, 1962, 686, 4518, 100, 102], True, ['ä½ ', 'å¥½', 'ä¸–', 'ç•Œ']),
        ([101, 872, 1962, 686, 4518, 100, 102], False, ['[CLS]', 'ä½ ', 'å¥½', 'ä¸–', 'ç•Œ', '[UNK]', '[SEP]']),
        ([101, 8701, 8572, 102], True, ['hello', 'world']),
        ([101, 8701, 8572, 102], False, ['[CLS]', 'hello', 'world', '[SEP]'])
    ]
)
def test_wordpiece_decode(test_input, skip_special_tokens, expected):
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)

    assert tokenizer.decode(test_input, skip_special_tokens=skip_special_tokens) == expected


@pytest.mark.parametrize(
    "left_text,right_text,expected",
    [
        (
            ''.join(['å¤´'] + ['[UNK]'] * 2),
            ''.join(['å¤´'] + ['[UNK]'] * 2),
            Encoding(
                ids=[101, 1928, 100, 100, 102, 1928, 100, 100, 102],
                segment_ids=[0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'å¤´', '[UNK]', '[UNK]', '[SEP]', 'å¤´', '[UNK]', '[UNK]', '[SEP]'],
            )
        ),
        (
            ''.join(['å¤´'] + ['[UNK]'] * 2),
            ''.join(['å¤´'] + ['[UNK]'] * 8),
            Encoding(
                ids=[101, 1928, 100, 100, 102, 1928, 100, 100, 100, 102],
                segment_ids=[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
                tokens=['[CLS]', 'å¤´', '[UNK]', '[UNK]', '[SEP]', 'å¤´', '[UNK]', '[UNK]', '[UNK]', '[SEP]']
            )
        ),
        (
            ''.join(['å¤´'] + ['[UNK]'] * 8),
            ''.join(['å¤´'] + ['[UNK]'] * 2),
            Encoding(
                ids=[101, 1928, 100, 100, 100, 102, 1928, 100, 100, 102],
                segment_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'å¤´', '[UNK]', '[UNK]', '[UNK]', '[SEP]', 'å¤´', '[UNK]', '[UNK]', '[SEP]']
            )
        ),
        (
            ''.join(['å¤´'] + ['[UNK]'] * 8),
            ''.join(['å¤´'] + ['[UNK]'] * 8),
            Encoding(
                ids=[101, 1928, 100, 100, 100, 102, 1928, 100, 100, 102],
                segment_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'å¤´', '[UNK]', '[UNK]', '[UNK]', '[SEP]', 'å¤´', '[UNK]', '[UNK]', '[SEP]'],
            )
        ),
    ]
)
def test_wordpiece_truncation_post(left_text, right_text, expected):
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)
    tokenizer.enable_truncation(10, strategy='post')

    ret = tokenizer.encode(left_text, right_text, return_array=False)
    assert ret.ids == expected.ids
    assert ret.segment_ids == expected.segment_ids
    assert ret.tokens == expected.tokens


@pytest.mark.parametrize(
    "left_text,right_text,expected",
    [
        (
            ''.join(['[UNK]'] * 2 + ['å°¾']),
            ''.join(['[UNK]'] * 2 + ['å°¾']),
            Encoding(
                ids=[101, 100, 100, 2227, 102, 100, 100, 2227, 102],
                segment_ids=[0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', '[UNK]', '[UNK]', 'å°¾', '[SEP]', '[UNK]', '[UNK]', 'å°¾', '[SEP]'],
            )
        ),
        (
            ''.join(['[UNK]'] * 2 + ['å°¾']),
            ''.join(['[UNK]'] * 8 + ['å°¾']),
            Encoding(
                ids=[101, 100, 100, 2227, 102, 100, 100, 100, 2227, 102],
                segment_ids=[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
                tokens=['[CLS]', '[UNK]', '[UNK]', 'å°¾', '[SEP]', '[UNK]', '[UNK]', '[UNK]', 'å°¾', '[SEP]']
            )
        ),
        (
            ''.join(['[UNK]'] * 8 + ['å°¾']),
            ''.join(['[UNK]'] * 2 + ['å°¾']),
            Encoding(
                ids=[101, 100, 100, 100, 2227, 102, 100, 100, 2227, 102],
                segment_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', '[UNK]', '[UNK]', '[UNK]', 'å°¾', '[SEP]', '[UNK]', '[UNK]', 'å°¾', '[SEP]']
            )
        ),
        (
            ''.join(['[UNK]'] * 8 + ['å°¾']),
            ''.join(['[UNK]'] * 8 + ['å°¾']),
            Encoding(
                ids=[101, 100, 100, 100, 2227, 102, 100, 100, 2227, 102],
                segment_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', '[UNK]', '[UNK]', '[UNK]', 'å°¾', '[SEP]', '[UNK]', '[UNK]', 'å°¾', '[SEP]'],
            )
        ),
    ]
)
def test_wordpiece_truncation_pre(left_text, right_text, expected):
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)
    tokenizer.enable_truncation(10, strategy='pre')

    ret = tokenizer.encode(left_text, right_text, return_array=False)
    assert ret.ids == expected.ids
    assert ret.segment_ids == expected.segment_ids
    assert ret.tokens == expected.tokens


@pytest.mark.parametrize(
    'test_input,start_index,end_index,expected',
    [
        (
            'I like apples.', 3, 4, 'apples'
        ),
        (
            'æˆ‘å¥½èœå•Š!!', 1, 3, 'æˆ‘å¥½èœ'
        )
    ]
)
def test_wordpiece_tokens_mapping(test_input, start_index, end_index, expected):
    tokenizer = WPTokenizer(vocab_path=os.path.join(data_dir, 'wp_cn_vocab.txt'), lowercase=True)
    pred = tokenizer.encode(test_input)
    mapping = tokenizer.tokens_mapping(test_input, pred.tokens)
    assert test_input[mapping[start_index][0]: mapping[end_index][1]] == expected


@pytest.mark.parametrize(
    "left_text,right_text,expected",
    [
        (
            'I like apples', None,
            Encoding(
                [2, 31, 101, 4037, 18, 3],
                [0, 0, 0, 0, 0, 0],
                ['[CLS]', 'â–i', 'â–like', 'â–apple', 's', '[SEP]']
            )
        ),
        (
            'hello world', 'hello world!!!\nğŸ‘‹',
            Encoding(
                [2, 10975, 126, 3, 10975, 126, 28116, 13, 1, 3],
                [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
                ['[CLS]', 'â–hello', 'â–world', '[SEP]', 'â–hello', 'â–world', '!!!', 'â–', 'ğŸ‘‹', '[SEP]']
            )
        ),
    ]
)
def test_sentencepiece_encode(left_text, right_text, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'), lowercase=True)
    ret = tokenizer.encode(left_text, right_text, return_array=False)

    assert ret.ids == expected.ids
    assert ret.segment_ids == expected.segment_ids
    assert ret.tokens == expected.tokens


@pytest.mark.parametrize(
    "test_input,skip_special_tokens,expected",
    [
        ([2, 10975, 126, 3], True, ['â–hello', 'â–world']),
        ([2, 10975, 126, 3], False, ['[CLS]', 'â–hello', 'â–world', '[SEP]']),
        ([2, 10975, 126, 28116, 13, 3], True, ['â–hello', 'â–world', '!!!', 'â–']),
        ([2, 10975, 126, 28116, 13, 3], False, ['[CLS]', 'â–hello', 'â–world', '!!!', 'â–', '[SEP]'])
    ]
)
def test_sentencepiece_decode(test_input, skip_special_tokens, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'))
    
    assert tokenizer.decode(test_input, skip_special_tokens=skip_special_tokens) == expected


@pytest.mark.parametrize(
    "left_text,right_text,expected",
    [
        (
            ' '.join(['head'] + ['unknown'] * 2),
            ' '.join(['head'] + ['unknown'] * 2),
            Encoding(
                ids=[2, 157, 2562, 2562, 3, 157, 2562, 2562, 3],
                segment_ids=[0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'â–head', 'â–unknown', 'â–unknown', '[SEP]', 'â–head', 'â–unknown', 'â–unknown', '[SEP]'],
            )
        ),
        (
            ' '.join(['head'] + ['unknown'] * 2),
            ' '.join(['head'] + ['unknown'] * 8),
            Encoding(
                ids=[2, 157, 2562, 2562, 3, 157, 2562, 2562, 2562, 3],
                segment_ids=[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
                tokens=['[CLS]', 'â–head', 'â–unknown', 'â–unknown', '[SEP]', 'â–head', 'â–unknown', 'â–unknown', 'â–unknown', '[SEP]']
            )
        ),
        (
            ' '.join(['head'] + ['unknown'] * 8),
            ' '.join(['head'] + ['unknown'] * 2),
            Encoding(
                ids=[2, 157, 2562, 2562, 2562, 3, 157, 2562, 2562, 3],
                segment_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'â–head', 'â–unknown', 'â–unknown', 'â–unknown', '[SEP]', 'â–head', 'â–unknown', 'â–unknown', '[SEP]']
            )
        ),
        (
            ' '.join(['head'] + ['unknown'] * 8),
            ' '.join(['head'] + ['unknown'] * 8),
            Encoding(
                ids=[2, 157, 2562, 2562, 2562, 3, 157, 2562, 2562, 3],
                segment_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'â–head', 'â–unknown', 'â–unknown', 'â–unknown', '[SEP]', 'â–head', 'â–unknown', 'â–unknown', '[SEP]'],
            )
        ),
    ]
)
def test_sentencepiece_truncation_post(left_text, right_text, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'))
    tokenizer.enable_truncation(10, strategy='post')

    ret = tokenizer.encode(left_text, right_text, return_array=False)
    assert ret.ids == expected.ids
    assert ret.segment_ids == expected.segment_ids
    assert ret.tokens == expected.tokens


@pytest.mark.parametrize(
    "left_text,right_text,expected",
    [
        (
            ' '.join(['unknown'] * 2 + ['tail']),
            ' '.join(['unknown'] * 2 + ['tail']),
            Encoding(
                ids=[2, 2562, 2562, 3424, 3, 2562, 2562, 3424, 3],
                segment_ids=[0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'â–unknown', 'â–unknown', 'â–tail', '[SEP]', 'â–unknown', 'â–unknown', 'â–tail', '[SEP]'],
            )
        ),
        (
            ' '.join(['unknown'] * 2 + ['tail']),
            ' '.join(['unknown'] * 8 + ['tail']),
            Encoding(
                ids=[2, 2562, 2562, 3424, 3, 2562, 2562, 2562, 3424, 3],
                segment_ids=[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
                tokens=['[CLS]', 'â–unknown', 'â–unknown', 'â–tail', '[SEP]', 'â–unknown', 'â–unknown', 'â–unknown', 'â–tail', '[SEP]']
            )
        ),
        (
            ' '.join(['unknown'] * 8 + ['tail']),
            ' '.join(['unknown'] * 2 + ['tail']),
            Encoding(
                ids=[2, 2562, 2562, 2562, 3424, 3, 2562, 2562, 3424, 3],
                segment_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'â–unknown', 'â–unknown', 'â–unknown', 'â–tail', '[SEP]', 'â–unknown', 'â–unknown', 'â–tail', '[SEP]']
            )
        ),
        (
            ' '.join(['unknown'] * 8 + ['tail']),
            ' '.join(['unknown'] * 8 + ['tail']),
            Encoding(
                ids=[2, 2562, 2562, 2562, 3424, 3, 2562, 2562, 3424, 3],
                segment_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                tokens=['[CLS]', 'â–unknown', 'â–unknown', 'â–unknown', 'â–tail', '[SEP]', 'â–unknown', 'â–unknown', 'â–tail', '[SEP]'],
            )
        ),
    ]
)
def test_sentencepiece_truncation_pre(left_text, right_text, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'))
    tokenizer.enable_truncation(10, strategy='pre')

    ret = tokenizer.encode(left_text, right_text, return_array=False)
    assert ret.ids == expected.ids
    assert ret.segment_ids == expected.segment_ids
    assert ret.tokens == expected.tokens


@pytest.mark.parametrize(
    'inputs,padding_strategy,expected',
    [
        (
            [('I like apples', 'I like watermelones'), ('hello world!', 'hello world')],
            'post',
            Encoding(
                ids=[[2, 13, 1, 101, 4037, 18, 3, 13, 1, 101, 308, 21008, 2696, 3],
                     [2, 10975, 126, 187, 3, 10975, 126, 3, 0, 0, 0, 0, 0, 0]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]],
                tokens=[['[CLS]', 'â–', 'I', 'â–like', 'â–apple', 's', '[SEP]', 'â–', 'I', 'â–like', 'â–water', 'melo', 'nes', '[SEP]'],
                        ['[CLS]', 'â–hello', 'â–world', '!', '[SEP]', 'â–hello', 'â–world', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]  
            ),
        ), (
            [('I like apples', 'I like watermelones'), ('hello world!', 'hello world')],
            'pre',
            Encoding(
                ids=[[2, 13, 1, 101, 4037, 18, 3, 13, 1, 101, 308, 21008, 2696, 3],
                     [0, 0, 0, 0, 0, 0, 2, 10975, 126, 187, 3, 10975, 126, 3]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]],
                tokens=[['[CLS]', 'â–', 'I', 'â–like', 'â–apple', 's', '[SEP]', 'â–', 'I', 'â–like', 'â–water', 'melo', 'nes', '[SEP]'],
                        ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '[CLS]', 'â–hello', 'â–world', '!', '[SEP]', 'â–hello', 'â–world', '[SEP]']]
            )
        )
    ]
)
def test_sentencepiece_batch_encode_padding(inputs, padding_strategy, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'))
    pred = tokenizer.encode_batch(inputs, padding=True, padding_strategy=padding_strategy, return_array=False)
    assert pred.ids == expected.ids
    assert pred.segment_ids == expected.segment_ids
    assert pred.tokens == expected.tokens


@pytest.mark.parametrize(
    'inputs,expected',
    [
        (
            [('I like apples', 'I like watermelones'), ('hello world!', 'hello world')],
            Encoding(
                ids=[[2, 13, 1, 101, 4037, 18, 3, 13, 1, 101, 308, 21008, 2696, 3],
                     [2, 10975, 126, 187, 3, 10975, 126, 3]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 1, 1, 1]],
                tokens=[['[CLS]', 'â–', 'I', 'â–like', 'â–apple', 's', '[SEP]', 'â–', 'I', 'â–like', 'â–water', 'melo', 'nes', '[SEP]'],
                        ['[CLS]', 'â–hello', 'â–world', '!', '[SEP]', 'â–hello', 'â–world', '[SEP]']]
            ),
        )
    ]
)
def test_sentencepiece_batch_encode_no_padding(inputs, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'))
    pred = tokenizer.encode_batch(inputs, padding=False, return_array=False)
    assert pred.ids == expected.ids
    assert pred.segment_ids == expected.segment_ids
    assert pred.tokens == expected.tokens


@pytest.mark.parametrize(
    'inputs,padding_strategy,expected',
    [
        (
            [('I like apples', 'I like watermelones'), ('hello world!', 'hello world')],
            'post',
            Encoding(
                ids=[[2, 13, 1, 101, 4037, 3, 13, 1, 101, 3],
                     [2, 10975, 126, 187, 3, 10975, 126, 3, 0, 0]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 1, 1, 1, 0, 0]],
                tokens=[['[CLS]', 'â–', 'I', 'â–like', 'â–apple', '[SEP]', 'â–', 'I', 'â–like', '[SEP]'],
                        ['[CLS]', 'â–hello', 'â–world', '!', '[SEP]', 'â–hello', 'â–world', '[SEP]', '<pad>', '<pad>']]
            ),
        ),
        (
            [('I like apples', 'I like watermelones'), ('hello world!', 'hello world')],
            'pre',
            Encoding(
                ids=[[2, 13, 1, 101, 4037, 3, 13, 1, 101, 3],
                     [0, 0, 2, 10975, 126, 187, 3, 10975, 126, 3]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
                             [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]],
                tokens=[['[CLS]', 'â–', 'I', 'â–like', 'â–apple', '[SEP]', 'â–', 'I', 'â–like', '[SEP]'],
                        ['<pad>', '<pad>', '[CLS]', 'â–hello', 'â–world', '!', '[SEP]', 'â–hello', 'â–world', '[SEP]']]
            ),
        )
    ]
)
def test_sentencepiece_batch_encode_truncation(inputs, padding_strategy, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'))
    tokenizer.enable_truncation(max_length=10)
    pred = tokenizer.encode_batch(inputs, padding_strategy=padding_strategy, return_array=False)
    assert pred.ids == expected.ids
    assert pred.segment_ids == expected.segment_ids
    assert pred.tokens == expected.tokens


@pytest.mark.parametrize(
    'inputs,padding_strategy,expected',
    [
        (
            ['I like apples', 'I like watermelones', 'hello world!'],
            'post',
            Encoding(
                ids=[[2, 13, 1, 101, 4037, 18, 3, 0],
                     [2, 13, 1, 101, 308, 21008, 2696, 3],
                     [2, 10975, 126, 187, 3, 0, 0, 0]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0]],
                tokens=[['[CLS]', 'â–', 'I', 'â–like', 'â–apple', 's', '[SEP]', '<pad>'],
                        ['[CLS]', 'â–', 'I', 'â–like', 'â–water', 'melo', 'nes', '[SEP]'],
                        ['[CLS]', 'â–hello', 'â–world', '!', '[SEP]', '<pad>', '<pad>', '<pad>']]
            ),
        ), (
            ['I like apples', 'I like watermelones', 'hello world!'],
            'pre',
            Encoding(
                ids=[[0, 2, 13, 1, 101, 4037, 18, 3],
                     [2, 13, 1, 101, 308, 21008, 2696, 3],
                     [0, 0, 0, 2, 10975, 126, 187, 3]],
                segment_ids=[[0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0]],
                tokens=[['<pad>', '[CLS]', 'â–', 'I', 'â–like', 'â–apple', 's', '[SEP]'],
                        ['[CLS]', 'â–', 'I', 'â–like', 'â–water', 'melo', 'nes', '[SEP]'],
                        ['<pad>', '<pad>', '<pad>', '[CLS]', 'â–hello', 'â–world', '!', '[SEP]']]
            )
        )
    ]
)
def test_sentencepiece_batch_encode_single(inputs, padding_strategy, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'))
    pred = tokenizer.encode_batch(inputs, padding=True, padding_strategy=padding_strategy, return_array=False)
    assert pred.ids == expected.ids
    assert pred.segment_ids == expected.segment_ids
    assert pred.tokens == expected.tokens


@pytest.mark.parametrize(
    'test_input,start_index,end_index,expected',
    [
        (
            'I like watermelons.', 3, 5, 'watermelons'
        ),
        (
            'Hello world!!!', 1, 1, 'Hello'
        )
    ]
)
def test_sentencepiece_tokens_mapping(test_input, start_index, end_index, expected):
    tokenizer = SPTokenizer(vocab_path=os.path.join(data_dir, 'albert_vocab/30k-clean.model'), lowercase=True)
    pred = tokenizer.encode(test_input)
    mapping = tokenizer.tokens_mapping(test_input, pred.tokens)
    assert test_input[mapping[start_index][0]: mapping[end_index][1]] == expected
